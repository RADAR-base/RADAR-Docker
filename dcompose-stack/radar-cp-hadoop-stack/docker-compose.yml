---
version: '2.3'

networks:
  zookeeper:
    driver: bridge
    internal: true
  kafka:
    driver: bridge
    internal: true
  api:
    driver: bridge
    internal: true
  mail:
    driver: bridge
    internal: true
  monitor:
    driver: bridge
    internal: true
  hotstorage:
    driver: bridge
    internal: true
  management:
    driver: bridge
    internal: true
    # driver_opts:
    #     com.docker.network.driver.mtu: 1450
  redis:
    driver: bridge
    internal: true
  hadoop:
    external: true

volumes:
  kafka-1-data: {}
  kafka-2-data: {}
  kafka-3-data: {}
  zookeeper-1-data: {}
  zookeeper-2-data: {}
  zookeeper-3-data: {}
  zookeeper-1-log: {}
  zookeeper-2-log: {}
  zookeeper-3-log: {}
  radar-backend-monitor-disconnect-data: {}
  redis-data: {}
  certs:
    external: true
  certs-data:
    external: true

services:
  #---------------------------------------------------------------------------#
  # Zookeeper Cluster                                                         #
  #---------------------------------------------------------------------------#
  zookeeper-1:
    image: confluentinc/cp-zookeeper:4.1.0
    networks:
      - zookeeper
    volumes:
      - zookeeper-1-data:/var/lib/zookeeper/data
      - zookeeper-1-log:/var/lib/zookeeper/log
    restart: always
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 10
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    healthcheck:
      test: ["CMD", "/bin/bash", "-c", "[ $$(echo dump | nc zookeeper-1 2181 | head -c1 | wc -c) -gt 0 ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  zookeeper-2:
    image: confluentinc/cp-zookeeper:4.1.0
    networks:
      - zookeeper
    volumes:
      - zookeeper-2-data:/var/lib/zookeeper/data
      - zookeeper-2-log:/var/lib/zookeeper/log
    restart: always
    environment:
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 10
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    healthcheck:
      test: ["CMD", "/bin/bash", "-c", "[ $$(echo dump | nc zookeeper-2 2181 | head -c1 | wc -c) -gt 0 ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  zookeeper-3:
    image: confluentinc/cp-zookeeper:4.1.0
    networks:
      - zookeeper
    volumes:
      - zookeeper-3-data:/var/lib/zookeeper/data
      - zookeeper-3-log:/var/lib/zookeeper/log
    restart: always
    environment:
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 10
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    healthcheck:
      test: ["CMD", "/bin/bash", "-c", "[ $$(echo dump | nc zookeeper-3 2181 | head -c1 | wc -c) -gt 0 ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # Kafka Cluster                                                             #
  #---------------------------------------------------------------------------#
  kafka-1:
    image: confluentinc/cp-kafka:4.1.0
    networks:
      - kafka
      - zookeeper
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    restart: always
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 730
      KAFKA_MESSAGE_MAX_BYTES: 4000048
      KAFKA_LOG4J_LOGGERS: kafka.producer.async.DefaultEventHandler=INFO,kafka.controller=INFO,state.change.logger=INFO
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: "1.1"
      KAFKA_LOG_MESSAGE_FORMAT_VERSION: "1.1"
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080
      KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS: 18000
      KAFKA_REPLICA_LAG_TIME_MS: 30000
    healthcheck:
      test: ["CMD-SHELL", "echo dump | nc zookeeper-1 2181 | grep -q /brokers/ids/1 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  kafka-2:
    image: confluentinc/cp-kafka:4.1.0
    networks:
      - kafka
      - zookeeper
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    restart: always
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 730
      KAFKA_MESSAGE_MAX_BYTES: 4000048
      KAFKA_LOG4J_LOGGERS: kafka.producer.async.DefaultEventHandler=INFO,kafka.controller=INFO,state.change.logger=INFO
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: "1.1"
      KAFKA_LOG_MESSAGE_FORMAT_VERSION: "1.1"
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080
      KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS: 18000
      KAFKA_REPLICA_LAG_TIME_MS: 30000
    healthcheck:
      test: ["CMD-SHELL", "echo dump | nc zookeeper-1 2181 | grep -q /brokers/ids/2 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  kafka-3:
    image: confluentinc/cp-kafka:4.1.0
    networks:
      - kafka
      - zookeeper
    volumes:
      - kafka-3-data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    restart: always
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 730
      KAFKA_MESSAGE_MAX_BYTES: 4000048
      KAFKA_LOG4J_LOGGERS: kafka.producer.async.DefaultEventHandler=INFO,kafka.controller=INFO,state.change.logger=INFO
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: "1.1"
      KAFKA_LOG_MESSAGE_FORMAT_VERSION: "1.1"
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080
      KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS: 18000
      KAFKA_REPLICA_LAG_TIME_MS: 30000
    healthcheck:
      test: ["CMD-SHELL", "echo dump | nc zookeeper-1 2181 | grep -q /brokers/ids/3 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  #---------------------------------------------------------------------------#
  # Schema Registry                                                           #
  #---------------------------------------------------------------------------#
  schema-registry-1:
    image: confluentinc/cp-schema-registry:4.1.0
    networks:
      - kafka
      - api
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    restart: always
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,PLAINTEXT://kafka-2:9092,PLAINTEXT://kafka-3:9092
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8081/subjects"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # REST proxy                                                                #
  #---------------------------------------------------------------------------#
  rest-proxy-1:
    image: confluentinc/cp-kafka-rest:4.1.0
    networks:
      - kafka
      - zookeeper
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
    restart: always
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_REST_LISTENERS: http://0.0.0.0:8082
      KAFKA_REST_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KAFKA_REST_HOST_NAME: rest-proxy-1
      KAFKA_REST_COMPRESSION_TYPE: lz4
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8082/topics"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # Kafka Init                                                                #
  #---------------------------------------------------------------------------#
  kafka-init:
    image: radarbase/radar-schemas-tools:${RADAR_SCHEMAS_VERSION}
    restart: "no" # On start failure, users need to run "install" again
    networks:
      - kafka
      - zookeeper
    command: "topic_init.sh"
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
    volumes:
      - ./etc/schema:/schema/conf
    environment:
      KAFKA_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,PLAINTEXT://kafka-2:9092,PLAINTEXT://kafka-3:9092
      KAFKA_SCHEMA_REGISTRY: http://schema-registry-1:8081
      KAFKA_NUM_BROKERS: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_NUM_REPLICATION: 3
      TOPIC_INIT_TRIES: ${TOPIC_INIT_TRIES}

  #---------------------------------------------------------------------------#
  # RADAR Hot Storage                                                         #
  #---------------------------------------------------------------------------#
  hotstorage:
    image: radarbase/radar-hotstorage:0.1
    networks:
      - hotstorage
    volumes:
      - "${MONGODB_DIR}/db:/data/db"
      - "${MONGODB_DIR}/configdb:/data/configdb"
    restart: always
    environment:
      RADAR_USER: ${HOTSTORAGE_USERNAME}
      RADAR_PWD: ${HOTSTORAGE_PASSWORD}
      RADAR_DB: ${HOTSTORAGE_NAME}
    healthcheck:
      test: ["CMD", "mongo", "-u", "${HOTSTORAGE_USERNAME}", "-p", "${HOTSTORAGE_PASSWORD}", "${HOTSTORAGE_NAME}", "--eval", "db"]
      interval: 1m
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR REST API                                                            #
  #---------------------------------------------------------------------------#
  rest-api:
    image: radarbase/radar-restapi:0.3
    networks:
      - hotstorage
      - api
      - management
    depends_on:
      - hotstorage
      - managementportal-app
    restart: always
    volumes:
      - "./etc/rest-api:/usr/local/conf/radar/rest-api"
    environment:
      RADAR_IS_CONFIG_LOCATION: usr/local/conf/radar/rest-api/radar-is.yml
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:8080/api/openapi.json"]
      interval: 1m
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR Dashboard                                                           #
  #---------------------------------------------------------------------------#
  dashboard:
    image: radarcns/radar-dashboard:2.1.0
    networks:
      - api
    depends_on:
      - rest-api
    restart: always
    environment:
      API_URI: https://${SERVER_NAME}/api
      BASE_HREF: /dashboard/
    healthcheck:
      test: ["CMD", "wget", "-s", "http://localhost:80/"]
      interval: 1m
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR Cold Storage                                                        #
  #---------------------------------------------------------------------------#
  hdfs-datanode-1:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    hostname: hdfs-datanode-1
    command: datanode
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
    volumes:
      - "${HDFS_DATA_DIR_1}:/hadoop/dfs/data"
    restart: always
    environment:
      SERVICE_9866_NAME: datanode
      SERVICE_9867_IGNORE: "true"
      SERVICE_9864_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_REPLICATION: 2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  hdfs-datanode-2:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    command: datanode
    hostname: hdfs-datanode-2
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
    volumes:
      - "${HDFS_DATA_DIR_2}:/hadoop/dfs/data"
    restart: always
    environment:
      SERVICE_9866_NAME: datanode
      SERVICE_9867_IGNORE: "true"
      SERVICE_9864_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_REPLICATION: 2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  hdfs-datanode-3:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    command: datanode
    hostname: hdfs-datanode-3
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
    volumes:
      - "${HDFS_DATA_DIR_3}:/hadoop/dfs/data"
    restart: always
    environment:
      SERVICE_9866_NAME: datanode
      SERVICE_9867_IGNORE: "true"
      SERVICE_9864_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_REPLICATION: 2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  hdfs-namenode-1:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    command: namenode-1
    hostname: hdfs-namenode-1
    networks:
      - hadoop
    volumes:
      - "${HDFS_NAME_DIR_1}:/hadoop/dfs/name/1"
      - "${HDFS_NAME_DIR_2}:/hadoop/dfs/name/2"
    restart: always
    environment:
      SERVICE_8020_NAME: namenode
      SERVICE_9870_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_NAME_DIR: file:///hadoop/dfs/name/1,file:///hadoop/dfs/name/2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  #---------------------------------------------------------------------------#
  # Email server                                                              #
  #---------------------------------------------------------------------------#
  smtp:
    image: namshi/smtp:latest
    restart: always
    networks:
      - mail
      - default
    volumes:
      - /var/spool/exim
    env_file:
      - ./etc/smtp.env
    command: ["/bin/sh", "-c", "apt-get update && apt-get install -y wget && exim -bd -q15m -v"]
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:25"]
      interval: 1m
      timeout: 15s
      retries: 3
      start_period: 30s

  #---------------------------------------------------------------------------#
  # RADAR mongo connector                                                     #
  #---------------------------------------------------------------------------#
  radar-mongodb-connector:
    image: radarbase/kafka-connect-mongodb-sink:0.2.2
    restart: on-failure
    volumes:
      - ./etc/mongodb-connector/sink-mongo.properties:/etc/kafka-connect/sink.properties
    networks:
      - zookeeper
      - kafka
      - hotstorage
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
      - rest-proxy-1
      - kafka-init
      - hotstorage
    environment:
      CONNECT_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,PLAINTEXT://kafka-2:9092,PLAINTEXT://kafka-3:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "default"
      CONNECT_CONFIG_STORAGE_TOPIC: "default.config"
      CONNECT_OFFSET_STORAGE_TOPIC: "default.offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "default.status"
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry-1:8081"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry-1:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_OFFSET_STORAGE_FILE_FILENAME: "/tmp/connect2.offset"
      CONNECT_REST_ADVERTISED_HOST_NAME: "radar-mongodb-connector"
      CONNECT_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      CONNECT_CONSUMER_MAX_POLL_RECORDS: 500
      CONNECT_CONSUMER_MAX_POLL_INTERVAL_MS: 300000
      CONNECT_CONSUMER_SESSION_TIMEOUT_MS: 10000
      CONNECT_CONSUMER_HEARTBEAT_INTERVAL_MS: 3000
      CONNECT_PLUGIN_PATH: /usr/share/java/kafka-connect/plugins
      KAFKA_BROKERS: 3
      CONNECT_LOG4J_ROOT_LOGLEVEL: WARN
      CONNECT_LOG4J_LOGGERS: "org.reflections=ERROR"
    healthcheck:
      test: ["CMD-SHELL", "curl  -sf localhost:8083/connectors/radar-connector-mongodb-sink/status | grep -o '\"state\":\"[^\"]*\"' | tr '\\n' ',' | grep -vq FAILED || exit 1"]
      interval: 1m
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR HDFS connector                                                      #
  #---------------------------------------------------------------------------#
  radar-hdfs-connector:
    image: radarbase/radar-connect-hdfs-sink:0.2.1
    restart: on-failure
    volumes:
      - ./etc/hdfs-connector/sink-hdfs.properties:/etc/kafka-connect/sink-hdfs.properties
    networks:
      - zookeeper
      - kafka
      - hadoop
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
      - kafka-init
      - hdfs-datanode-1
      - hdfs-datanode-2
      - hdfs-datanode-3
      - hdfs-namenode-1
    environment:
      CONNECT_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,PLAINTEXT://kafka-2:9092,PLAINTEXT://kafka-3:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "default"
      CONNECT_CONFIG_STORAGE_TOPIC: "default.config"
      CONNECT_OFFSET_STORAGE_TOPIC: "default.offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "default.status"
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry-1:8081"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry-1:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_OFFSET_STORAGE_FILE_FILENAME: "/tmp/connect2.offset"
      CONNECT_REST_ADVERTISED_HOST_NAME: "radar-hdfs-connector"
      CONNECT_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      CONNECTOR_PROPERTY_FILE_PREFIX: "sink-hdfs"
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx768m"
      KAFKA_BROKERS: 3
      CONNECT_LOG4J_LOGGERS: "org.reflections=ERROR"
    healthcheck:
      test: ["CMD-SHELL", "curl  -sf localhost:8083/connectors/radar-hdfs-sink-android-15000/status | grep -o '\"state\":\"[^\"]*\"' | tr '\\n' ',' | grep -vq FAILED || exit 1"]
      interval: 1m
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR backend streams                                                     #
  #---------------------------------------------------------------------------#
  radar-backend-stream:
    image: radarbase/radar-backend:0.4.0
    command:
      - stream
    networks:
      - zookeeper
      - kafka
      # for getting the play store category
      - default
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
      - kafka-init
    volumes:
      - ./etc/radar-backend/radar.yml:/etc/radar.yml
    restart: always
    environment:
      KAFKA_REST_PROXY: http://rest-proxy-1:8082
      KAFKA_SCHEMA_REGISTRY: http://schema-registry-1:8081
      KAFKA_BROKERS: 3

  #---------------------------------------------------------------------------#
  # RADAR backend monitor                                                     #
  #---------------------------------------------------------------------------#
  radar-backend-monitor:
    image: radarbase/radar-backend:0.4.0
    command: monitor
    networks:
      - zookeeper
      - kafka
      - mail
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
      - kafka-init
      - smtp
    volumes:
      - ./etc/radar-backend/radar.yml:/etc/radar.yml
      - radar-backend-monitor-disconnect-data:/var/lib/radar/data
    restart: always
    environment:
      KAFKA_REST_PROXY: http://rest-proxy-1:8082
      KAFKA_SCHEMA_REGISTRY: http://schema-registry-1:8081
      KAFKA_BROKERS: 3
      # For backwards compatibility
      TOPIC_LIST: "application_record_counts"

  #---------------------------------------------------------------------------#
  # Docker Monitoring                                                         #
  #---------------------------------------------------------------------------#
  portainer:
    image: portainer/portainer:1.22.0
    command: --admin-password '${PORTAINER_PASSWORD_HASH}'
    networks:
      - monitor
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    restart: always

  #---------------------------------------------------------------------------#
  # Webserver                                                                 #
  #---------------------------------------------------------------------------#
  webserver:
    image: nginx:1.14.0-alpine
    restart: always
    networks:
      - api
      - monitor
      - default
    depends_on:
      - portainer
      - rest-api
      - schema-registry-1
      - gateway
      - dashboard
      - managementportal-app
      - kafka-manager
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - certs:/etc/letsencrypt
      - certs-data:/data/letsencrypt
      - "./etc/webserver/nginx.conf:/etc/nginx/nginx.conf:ro"
      - "./etc/webserver/cors.conf:/etc/nginx/cors.conf:ro"
      - "./etc/webserver/ip-access-control.conf:/etc/nginx/ip-access-control.conf:ro"
      - "./etc/webserver/kafka-manager.htpasswd:/etc/nginx/kafka-manager.htpasswd:ro"
      - "./etc/webserver/optional-services.conf:/etc/nginx/optional-services.conf"
    # healthcheck hard to do, however, it is possible to monitor this externally
    # with
    # docker logs --since 2m radarcphadoopstack_webserver_1 | grep "connect() failed"

  #---------------------------------------------------------------------------#
  # Management Portal                                                         #
  #---------------------------------------------------------------------------#
  managementportal-app:
    image: radarbase/management-portal:0.6.5
    restart: always
    networks:
      - default
      - api
      - management
      - mail
    depends_on:
      radarbase-postgresql:
        condition: service_healthy
      smtp:
        condition: service_healthy
      catalog-server:
        condition: service_healthy
    environment:
      SPRING_PROFILES_ACTIVE: prod,swagger
      SPRING_DATASOURCE_URL: jdbc:postgresql://radarbase-postgresql:5432/managementportal
      SPRING_DATASOURCE_USERNAME: ${POSTGRES_USER}
      SPRING_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}
      MANAGEMENTPORTAL_MAIL_FROM: ${FROM_EMAIL}
      MANAGEMENTPORTAL_COMMON_BASEURL: https://${SERVER_NAME}/
      MANAGEMENTPORTAL_COMMON_MANAGEMENT_PORTAL_BASE_URL: https://${SERVER_NAME}/managementportal
      MANAGEMENTPORTAL_FRONTEND_CLIENT_SECRET: ${MANAGEMENTPORTAL_FRONTEND_CLIENT_SECRET}
      MANAGEMENTPORTAL_OAUTH_CLIENTS_FILE: /mp-includes/config/oauth_client_details.csv
      MANAGEMENTPORTAL_CATALOGUE_SERVER_ENABLE_AUTO_IMPORT: ${MANAGEMENTPORTAL_CATALOGUE_SERVER_ENABLE_AUTO_IMPORT}
      MANAGEMENTPORTAL_CATALOGUE_SERVER_SERVER_URL: http://catalog-server:9010/source-types
      MANAGEMENTPORTAL_COMMON_ADMIN_PASSWORD: ${MANAGEMENTPORTAL_COMMON_ADMIN_PASSWORD}
      MANAGEMENTPORTAL_COMMON_PRIVACY_POLICY_URL: ${MANAGEMENTPORTAL_COMMON_PRIVACY_POLICY_URL}
      MANAGEMENTPORTAL_OAUTH_META_TOKEN_TIMEOUT: PT2H
      SPRING_APPLICATION_JSON: '{"managementportal":{"oauth":{"checkingKeyAliases":["${MANAGEMENTPORTAL_OAUTH_CHECKING_KEY_ALIASES_0}","${MANAGEMENTPORTAL_OAUTH_CHECKING_KEY_ALIASES_1}"]}}}'
      JHIPSTER_SLEEP: 10 # gives time for the database to boot before the application
      JAVA_OPTS: -Xmx256m  # maximum heap size for the JVM running ManagementPortal, increase this as necessary
    volumes:
      - ./etc/managementportal/:/mp-includes/
    healthcheck:
      test: ["CMD", "wget", "--spider", "localhost:8080/managementportal/oauth/token_key"]
      interval: 1m30s
      timeout: 5s
      retries: 3


  radarbase-postgresql:
    build:
      context: ./images/postgres
      args:
        POSTGRES_VERSION: ${POSTGRES_VERSION}
    image: radarbase/postgres:${POSTGRES_VERSION}-1
    restart: always
    volumes:
      - "${MP_POSTGRES_DIR}/data/:/var/lib/postgresql/data/"
      - "./postgres-backup/backups/postgresql:/backups/database/postgresql/"
      - "./postgres-backup/scripts:/backup-scripts"
    environment:
      POSTGRES_USER : ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: managementportal,restsourceauthorizer
    networks:
      - management
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD='${POSTGRES_PASSWORD}' psql -U '${POSTGRES_USER}' managementportal -l || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # Kafka Manager                                                             #
  #---------------------------------------------------------------------------#
  kafka-manager:
    image: radarbase/kafka-manager:1.3.3.18
    networks:
      - zookeeper
      - kafka
      - api
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
    environment:
      ZK_HOSTS: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "[ $$(wget -q -O - localhost:9000/kafkamanager/api/health) = healthy ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR Gateway                                                             #
  #---------------------------------------------------------------------------#
  gateway:
    image: radarbase/radar-gateway:0.3.10
    restart: always
    networks:
      - api
      - kafka
    depends_on:
      - rest-proxy-1
    volumes:
      - ./etc/gateway:/etc/radar-gateway
    command: ["radar-gateway", "/etc/radar-gateway/gateway.yml"]
    healthcheck:
      # should give an unauthenticated response, rather than a 404
      test: ["CMD-SHELL", "curl --fail localhost/radar-gateway/topics 2>&1 | grep -q 401 || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # Catalog server from radar-schemas                                         #
  #---------------------------------------------------------------------------#
  catalog-server:
    image: radarbase/radar-schemas-tools:${RADAR_SCHEMAS_VERSION}
    restart: always
    networks:
      - management
    command: radar-catalog-server /schema/merged
    volumes:
      - ./etc/schema:/schema/conf
    environment:
      KAFKA_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,PLAINTEXT://kafka-2:9092,PLAINTEXT://kafka-3:9092
      KAFKA_SCHEMA_REGISTRY: http://schema-registry-1:8081
      KAFKA_NUM_BROKERS: 3
      RADAR_NUM_PARTITIONS: 3
      RADAR_NUM_REPLICATION_FACTOR: 3
      TOPIC_INIT_TRIES: ${TOPIC_INIT_TRIES}
    healthcheck:
        test: ["CMD", "curl", "-f", "localhost:9010/source-types"]
        interval: 1m30s
        timeout: 5s
        retries: 3

  radar-output:
    image: radarbase/radar-output-restructure:1.1.1-hdfs
    restart: always
    stop_signal: SIGINT
    networks:
      - redis
      - hadoop
    depends_on:
      - output-redis
      - hdfs-namenode-1
      - hdfs-datanode-1
      - hdfs-datanode-2
      - hdfs-datanode-3
    volumes:
      - ${RESTRUCTURE_OUTPUT_DIR}:/output
      - ./etc/hdfs-restructure/restructure.yml:/etc/restructure.yml
    environment:
      RADAR_HDFS_RESTRUCTURE_OPTS: -Xms250m -Xmx4g
    command: -F /etc/restructure.yml -S

  output-redis:
    image: bitnami/redis:5.0.8
    restart: always
    volumes:
      - redis-data:/data
    networks:
      - redis
    environment:
      ALLOW_EMPTY_PASSWORD: 'yes'
